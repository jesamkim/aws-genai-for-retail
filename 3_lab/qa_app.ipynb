{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161ee7b5-a3c5-48ca-9499-f413a6a27994",
   "metadata": {},
   "source": [
    "# Amazon Kendra를 이용한 \"매장 현장 직원을 위한 Q&A 챗봇 \n",
    "\n",
    "---\n",
    "\n",
    "이 노트북에서는 매장 현장 직원을 위한 Q&A 챗봇을 구현하기 위한 생성형 AI 기반의 대화식 답변(Generative Conversational Answers) 기능을 위해 [Amazon Kendra](https://aws.amazon.com/kendra/)과 [Amazon Bedrock](https://aws.amazon.com/bedrock/)를 사용합니다.\n",
    "\n",
    "완전관리형 지능형 검색서비스인 Amazon Kendra의 기능을 LLM과 결합하여 RAG 워크플로를 구현한다면 엔터프라이즈 콘텐츠에 대한 대화 경험을 제공하는 GenAI 애플리케이션을 쉽게 만들 수 있습니다. \n",
    "\n",
    "노트북은 아래와 같은 구성으로 이루어져있습니다.\n",
    "1. RAG 사용 사례를 위한 문서 업로드\n",
    "2. Amazon Kendra 검색엔진 사용을 위한 LangChain의 Amazon Kendra Retrieve API\n",
    "3. 웹앱을 쉽게 만들 수 있는 오픈 소스 Python 라이브러리 Streamlit을 이용한 챗봇 서비스 구현 \n",
    "\n",
    "본 노트북에서 사용되는 일반적인 리테일 회사에서 사용될 수 있는 가상의 문서를 참고했습니다.\n",
    "- 고객 응대 및 서비스 원칙\n",
    "- 근무 시간과 스케줄\n",
    "- 매장 업무 메뉴얼 목차\n",
    "- 상품 진열과 정리 원칙\n",
    "- 일일 근무 시간 및 주간 근무 일정 규정\n",
    "- 근태 규정\n",
    "- 팀원과 상사와의 협력\n",
    "\n",
    "Amazon Kendra와 Amazon Bedrock을 이용한 RAG 사례는 아래 다이어그램을 참조하세요.\n",
    "\n",
    "<img src=\"images/rag-architecture.png\" width=\"800\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed43b53-5576-4b94-919f-6d5c1f166d2f",
   "metadata": {},
   "source": [
    "## RAG 사례를 위한 Amazon S3 문서 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609ba3c-4743-49b4-acca-eab565df69a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install Microsoft Word (.docx) python library\n",
    "%pip install --quiet python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3ef03-949d-42fd-9173-14b89af5b7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f3008-7ac6-488f-b67b-f076804a218d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG에 이용될 업로드 문서 확인\n",
    "office_files = glob.glob(\"./data/*\")\n",
    "for filename in office_files:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c072131-df3a-489e-b74c-d51f28706bab",
   "metadata": {},
   "source": [
    "## 문서 확인\n",
    "\n",
    "---\n",
    "\n",
    "- 문서는 아래와 같은 내용으로 구성되어있습니다.\n",
    "\n",
    "  예) 상품 진열과 정리 원칙 (product_display.docx) 파일 일부\n",
    "\n",
    "<img src=\"images/docx_example.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf882e3e-2283-4c2d-a8a2-17eb504b61c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import docx \n",
    "\n",
    "document = docx.Document('./data/product_display.docx')\n",
    "for line, p in enumerate(document.paragraphs):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987815c-ace2-4d5f-a516-4f00610a4688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 문서 S3 업로드\n",
    "\n",
    "boto3 라이브러리를 이용해서 S3 버킷에 문서들을 모두 업로드힙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fe81f-379c-406c-acb4-335fa426fe64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STACK_NAME = 'genai-workshop'\n",
    "\n",
    "cf_client = boto3.client('cloudformation')\n",
    "response = cf_client.describe_stacks()\n",
    "\n",
    "for output in response[\"Stacks\"]:\n",
    "    stackName = output[\"StackName\"]\n",
    "    if stackName.find('Kendra') > 0:\n",
    "        response = cf_client.describe_stacks(StackName=stackName)\n",
    "        for output in response[\"Stacks\"][0][\"Outputs\"]:\n",
    "            keyName = output[\"OutputKey\"]\n",
    "            if keyName == \"S3Bucket\":\n",
    "                BUCKET_NAME = output[\"OutputValue\"]\n",
    "            if keyName == \"KendraIndex\":\n",
    "                KENDRA_INDEX = output[\"OutputValue\"]\n",
    "\n",
    "print('S3 Bucket Name: ', BUCKET_NAME)\n",
    "print('Kendra Index ID: ', KENDRA_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa66014-05c4-4155-a58f-a474e27b6536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "for filename in office_files:\n",
    "    key = os.path.basename(filename)\n",
    "    print(\"Putting \", filename,key)\n",
    "    s3.upload_file(filename, BUCKET_NAME, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091d822-4f56-44bd-a155-aa69e6a44fe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note: S3에 파일을 업로드할 경우 Kendra Index 동기화 이벤트가 발생되어 자동으로 인덱싱이 진행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7436f-cffa-4962-ba01-024e7b93df1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## LangChain의 RetrievalQA Chain을 이용\n",
    "\n",
    "LangChain이 Amazon Bedrock의 Claude-2 모델을 사용하도록 LLM 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476d60c-7cc3-4c76-8cf2-c88937e955bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet langchain==0.0.309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d26151-3769-44d2-b943-7e09ae42604d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_region = os.environ[\"AWS_DEFAULT_REGION\"]   # E.g. \"us-east-1\"\n",
    "os.environ[\"BEDROCK_ENDPOINT_URL\"] = f\"https://bedrock-runtime.{my_region}.amazonaws.com\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620aff6b-9591-4587-b8e5-06886b21d403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = os.environ[\"AWS_DEFAULT_REGION\"]  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"bedrock_claude\"\n",
    "os.environ[\"BEDROCK_ENDPOINT_URL\"] = os.environ[\"BEDROCK_ENDPOINT_URL\"]  # E.g. \"https://...\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    profile_name=os.environ.get(\"AWS_PROFILE\")\n",
    ") # sets the profile name to use for AWS credentials\n",
    "\n",
    "bedrock = session.client(\n",
    "    service_name='bedrock-runtime', # creates a Bedrock client\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\"),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\")\n",
    ") \n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# - create the Anthropic Model\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=bedrock, model_kwargs={'max_tokens_to_sample':1000, 'temperature': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9e2ab-dc2e-4b7d-9d62-32f46247fdef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## AmazonKendraRetriever 설정\n",
    "\n",
    "LangChain이 Kendra 검색 결과를 가져오기 위해 AmazonKendraRetriever 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1b8a9-f4a7-45be-9dc2-c5338ff2cd67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "\n",
    "retriever = AmazonKendraRetriever(\n",
    "    index_id=KENDRA_INDEX,\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    top_k=3,\n",
    "    attribute_filter = {\n",
    "        \"EqualsTo\": {      \n",
    "            \"Key\": \"_language_code\",\n",
    "            \"Value\": {\n",
    "                \"StringValue\": \"ko\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff69ae-35be-4234-ac67-43a006accc10",
   "metadata": {},
   "source": [
    "## Kendra Retriever로 문서 검색을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31622d5a-865a-4434-9c61-4f38dcd3d23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"상품 진열 방법\"의 Kendra 검색 결과 확인\n",
    "retriever.get_relevant_documents(\"상품 진열 방법\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3e60f-50d0-4cf5-bbdc-dba1912f61ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prompt Template 설정\n",
    "\n",
    "질문와 답변 형태의 대화를 위한 Prompt를 Anthroipc Claude의 Prompt format에 맞도록 아래와 같이 정의\n",
    "\n",
    "<li> 영어\n",
    "\n",
    "Human: This is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides specific details from its context but limits it to 240 tokens.\n",
    "If the AI does not know the answer to a question, it truthfully says it \n",
    "does not know.\n",
    "\n",
    "Assistant: OK, got it, I'll be a talkative truthful AI assistant.\n",
    "\n",
    "Human: Here are a few documents in <documents> tags:\n",
    "<documents>\n",
    "{context}\n",
    "</documents>\n",
    "Based on the above documents, provide a detailed answer for, {question} \n",
    "Answer **\"시스템에 관련된 정보가 없습니다.\"** if not present in the document. \n",
    "\n",
    "<li> 한글\n",
    "\n",
    "Assistant:\n",
    "    \n",
    "Human: 인간과 AI의 친근한 대화입니다.\n",
    "AI는 말이 많고 상황에 따른 구체적인 세부 정보를 제공하지만 토큰 수는 240개로 제한됩니다.\n",
    "AI가 질문에 대한 답을 모르면 알지 못한다고 사실대로 말합니다.\n",
    "\n",
    "Assistant: 알겠습니다. 저는 말이 많고 진실된 AI 어시스턴트가 되어 드리겠습니다.\n",
    "\n",
    "Human: 다음은 <documents> 태그에 있는 몇 가지 문서입니다.\n",
    "<문서>\n",
    "{문맥}\n",
    "</문서>\n",
    "위 문서를 바탕으로 {question}에 대한 자세한 답변을 제공해 주세요.\n",
    "문서에 관련된 내용이 없으면 \"시스템에 관련된 정보는 없습니다.\"라고 답변하세요.\n",
    "\n",
    "Assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633d2c5-78e2-4f58-a8a9-9f3a66cbd883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: This is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides specific details from its context but limits it to 240 tokens.\n",
    "If the AI does not know the answer to a question, it truthfully says it \n",
    "does not know.\n",
    "\n",
    "Assistant: OK, got it, I'll be a talkative truthful AI assistant.\n",
    "\n",
    "Human: Here are a few documents in <documents> tags:\n",
    "<documents>\n",
    "{context}\n",
    "</documents>\n",
    "Based on the above documents, provide a detailed answer for, {question} \n",
    "Answer \"시스템에 관련된 정보가 없습니다.\" if not present in the document. \n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a0505-fc08-455f-a98f-84ca2e29dfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # inserts all documents into a prompt and passes that prompt to an LLM\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e7744-1ab4-40b3-9834-6cd3ae82f058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# result = qa.run(question)\n",
    "query = \"상품 진열 방법 알려줘\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62511b1c-1116-44ed-894d-1c64d251493c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(result.strip())\n",
    "print(result['result'])\n",
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cc8db-7f73-49f0-8f0b-04f72b7a02d7",
   "metadata": {},
   "source": [
    "## 빠른 응답을 위해 claude-instant-v1 모델을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79ec44-fc13-4a82-9e14-dfc3d7e8cb13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=bedrock, model_kwargs={'max_tokens_to_sample':1000, 'temperature': 0})\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # inserts all documents into a prompt and passes that prompt to an LLM\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177740fb-a552-4393-b60a-0c9b1861e6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"FIFO는 무슨 뜻이야?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])\n",
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bef7ac-e0e4-4ef3-bc10-35721ee20f5c",
   "metadata": {},
   "source": [
    "## 위의 코드를 kendra_claude.py로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b043aa-8430-4c0f-8a00-94371de876f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store KENDRA_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77db2d-87b2-47ea-b602-8aea46660757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(KENDRA_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2ffab-28b7-4fca-b7f7-ec4e0759a56e",
   "metadata": {},
   "source": [
    "## LLM을 이용한 Q&A 챗봇 모듈을 kendra_claude.py로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cd3dc-7aad-400d-a953-00b48466be5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile kendra_claude.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "def build_chain():\n",
    "\n",
    "  session = boto3.Session(\n",
    "      profile_name=os.environ.get(\"AWS_PROFILE\")\n",
    "  ) \n",
    "  boto3_bedrock = session.client(\n",
    "    service_name='bedrock-runtime', \n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\"),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\")\n",
    "  ) \n",
    "    \n",
    "  region = os.environ[\"AWS_REGION\"]\n",
    "  kendra_index_id = \"<YOUR_KENDRA_INDEX>\" # Example: 65702b79-XXXX-XXXX-XXXX-9702f17fb994\n",
    "\n",
    "  llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':1000})\n",
    "  \n",
    "  retriever = AmazonKendraRetriever(\n",
    "    index_id=kendra_index_id,\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    top_k=3,\n",
    "    attribute_filter = {\n",
    "        \"EqualsTo\": {      \n",
    "            \"Key\": \"_language_code\",\n",
    "            \"Value\": {\n",
    "                \"StringValue\": \"ko\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "  )\n",
    "  # prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "  prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. If the answer is not in the context, just say \"시스템에 관련 내용을 찾을 수 없습니다.\", don't try to make up an answer.\n",
    "\n",
    "  {context}\n",
    "\n",
    "  Question: {question}\n",
    "  Assistant:\"\"\"\n",
    "\n",
    "  PROMPT = PromptTemplate(\n",
    "      template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "  )\n",
    "\n",
    "\n",
    "  qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    "  )\n",
    "\n",
    "  return qa\n",
    "\n",
    "def run_chain(chain, prompt: str):\n",
    "  return chain({\"query\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2374f4a-40c3-4ffb-a2b3-3ad5ea9d0f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kendra index 업데이트\n",
    "!sed -i 's/<YOUR_KENDRA_INDEX>/{KENDRA_INDEX}/g' kendra_claude.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9b965-a83d-4848-a103-5510962257a4",
   "metadata": {},
   "source": [
    "## Streamlit 애플리케이션 실행을 위한 app.py 파일을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd7e7e-680b-4471-8b39-890b438ab706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import sys\n",
    "\n",
    "import kendra_claude as claude\n",
    "\n",
    "USER_ICON = \"images/user-icon.png\"\n",
    "AI_ICON = \"images/ai-icon.png\"\n",
    "MAX_HISTORY_LENGTH = 5\n",
    "\n",
    "if 'llm_chain' not in st.session_state:\n",
    "    st.session_state['llm_app'] = claude\n",
    "    st.session_state['llm_chain'] = claude.build_chain()\n",
    "\n",
    "if 'chat_history' not in st.session_state:\n",
    "    st.session_state['chat_history'] = []\n",
    "    \n",
    "if \"chats\" not in st.session_state:\n",
    "    st.session_state.chats = [\n",
    "        {\n",
    "            'id': 0,\n",
    "            'question': '',\n",
    "            'answer': ''\n",
    "        }\n",
    "    ]\n",
    "\n",
    "if \"questions\" not in st.session_state:\n",
    "    st.session_state.questions = []\n",
    "\n",
    "if \"answers\" not in st.session_state:\n",
    "    st.session_state.answers = []\n",
    "\n",
    "if \"input\" not in st.session_state:\n",
    "    st.session_state.input = \"\"\n",
    "\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "        <style>\n",
    "               .block-container {\n",
    "                    padding-top: 32px;\n",
    "                    padding-bottom: 32px;\n",
    "                    padding-left: 0;\n",
    "                    padding-right: 0;\n",
    "                }\n",
    "                .element-container img {\n",
    "                    background-color: #000000;\n",
    "                }\n",
    "\n",
    "                .main-header {\n",
    "                    font-size: 24px;\n",
    "                }\n",
    "        </style>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "def write_logo():\n",
    "    col1, col2, col3 = st.columns([5, 1, 5])\n",
    "    with col2:\n",
    "        st.image(AI_ICON, use_column_width='always') \n",
    "\n",
    "\n",
    "def write_top_bar():\n",
    "    col1, col2, col3 = st.columns([1,10,2])\n",
    "    with col1:\n",
    "        st.image(AI_ICON, use_column_width='always')\n",
    "    with col2:\n",
    "        header = f\"Amazon Bedrock이 제공하는 AI 서비스!\"\n",
    "        st.write(f\"<h3 class='main-header'>{header}</h3>\", unsafe_allow_html=True)\n",
    "    with col3:\n",
    "        clear = st.button(\"Clear Chat\")\n",
    "    return clear\n",
    "\n",
    "clear = write_top_bar()\n",
    "\n",
    "if clear:\n",
    "    st.session_state.questions = []\n",
    "    st.session_state.answers = []\n",
    "    st.session_state.input = \"\"\n",
    "    st.session_state[\"chat_history\"] = []\n",
    "\n",
    "def handle_input():\n",
    "    input = st.session_state.input\n",
    "    question_with_id = {\n",
    "        'question': input,\n",
    "        'id': len(st.session_state.questions)\n",
    "    }\n",
    "    st.session_state.questions.append(question_with_id)\n",
    "\n",
    "    chat_history = st.session_state[\"chat_history\"]\n",
    "    if len(chat_history) == MAX_HISTORY_LENGTH:\n",
    "        chat_history = chat_history[:-1]\n",
    "\n",
    "    llm_chain = st.session_state['llm_chain']\n",
    "    chain = st.session_state['llm_app']\n",
    "    result = chain.run_chain(llm_chain, input)\n",
    "    answer = result['result']\n",
    "    chat_history.append((input, answer))\n",
    "    \n",
    "    document_list = []\n",
    "    if 'source_documents' in result:\n",
    "        for d in result['source_documents']:\n",
    "            if not (d.metadata['source'] in document_list):\n",
    "                document_list.append((d.metadata['source']))\n",
    "\n",
    "    st.session_state.answers.append({\n",
    "        'answer': result,\n",
    "        'sources': document_list,\n",
    "        'id': len(st.session_state.questions)\n",
    "    })\n",
    "    st.session_state.input = \"\"\n",
    "\n",
    "def write_user_message(md):\n",
    "    col1, col2 = st.columns([1,12])\n",
    "    \n",
    "    with col1:\n",
    "        st.image(USER_ICON, use_column_width='always')\n",
    "    with col2:\n",
    "        st.warning(md['question'])\n",
    "\n",
    "\n",
    "def render_result(result):\n",
    "    answer, sources = st.tabs(['Answer', 'Sources'])\n",
    "    with answer:\n",
    "        render_answer(result['answer'])\n",
    "    with sources:\n",
    "        if 'source_documents' in result:\n",
    "            render_sources(result['source_documents'])\n",
    "        else:\n",
    "            render_sources([])\n",
    "\n",
    "def render_answer(answer):\n",
    "    col1, col2 = st.columns([1,12])\n",
    "    with col1:\n",
    "        st.image(AI_ICON, use_column_width='always')\n",
    "    with col2:\n",
    "        st.info(answer['result'])\n",
    "\n",
    "def render_sources(sources):\n",
    "    col1, col2 = st.columns([1,12])\n",
    "    with col2:\n",
    "        with st.expander(\"Sources\"):\n",
    "            for s in sources:\n",
    "                st.write(s)\n",
    "\n",
    "    \n",
    "#Each answer will have context of the question asked in order to associate the provided feedback with the respective question\n",
    "def write_chat_message(md, q):\n",
    "    chat = st.container()\n",
    "    with chat:\n",
    "        render_answer(md['answer'])\n",
    "        render_sources(md['sources'])\n",
    "    \n",
    "        \n",
    "with st.container():\n",
    "  for (q, a) in zip(st.session_state.questions, st.session_state.answers):\n",
    "    write_user_message(q)\n",
    "    write_chat_message(a, q)\n",
    "\n",
    "st.markdown('---')\n",
    "input = st.text_input(\"질문을 해주세요!\", key=\"input\", on_change=handle_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5a5d6-a557-4e2a-a404-8f0f5efe2201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "boto3==1.28.64\n",
    "streamlit==1.20.0\n",
    "langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8d804-f5d0-4fb5-80a5-93bd69a54da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile setup.sh\n",
    "\n",
    "pip install --no-cache-dir -r requirements.txt\n",
    "sudo yum install -y iproute\n",
    "sudo yum install -y jq\n",
    "sudo yum install -y lsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a74374-6a63-404e-9501-c34dff704549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "\n",
    "#!/bin/sh\n",
    "CURRENTDATE=`date +\"%Y-%m-%d %T\"`\n",
    "RED='\\033[0;31m'\n",
    "CYAN='\\033[1;36m'\n",
    "GREEN='\\033[1;32m'\n",
    "NC='\\033[0m'\n",
    "S3_PATH=$1\n",
    "\n",
    "# Run the Streamlit app and save the output to \"temp.txt\"\n",
    "streamlit run app.py > temp.txt & \n",
    "\n",
    "# Read the text file using cat\n",
    "echo \"Getting the URL to view your Streamlit app in the browser\"\n",
    "\n",
    "# Extract the last four digits of the port number from the Network URL\n",
    "sleep 5\n",
    "PORT=$(grep \"Network URL\" temp.txt | awk -F':' '{print $NF}' | awk '{print $1}' | tail -c 5)\n",
    "echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Port Number ${PORT}\" \n",
    "\n",
    "\n",
    "\n",
    "# Get Studio domain information\n",
    "DOMAIN_ID=$(jq .DomainId /opt/ml/metadata/resource-metadata.json || exit 1)\n",
    "RESOURCE_NAME=$(jq .ResourceName /opt/ml/metadata/resource-metadata.json || exit 1)\n",
    "RESOURCE_ARN=$(jq .ResourceArn /opt/ml/metadata/resource-metadata.json || exit 1)\n",
    "\n",
    "# Remove quotes from string\n",
    "DOMAIN_ID=`sed -e 's/^\"//' -e 's/\"$//' <<< \"$DOMAIN_ID\"`\n",
    "RESOURCE_NAME=`sed -e 's/^\"//' -e 's/\"$//' <<< \"$RESOURCE_NAME\"`\n",
    "RESOURCE_ARN=`sed -e 's/^\"//' -e 's/\"$//' <<< \"$RESOURCE_ARN\"`\n",
    "RESOURCE_ARN_ARRAY=($(echo \"$RESOURCE_ARN\" | tr ':' '\\n'))\n",
    "\n",
    "# Get Studio domain region\n",
    "REGION=$(echo \"${RESOURCE_ARN_ARRAY[3]}\")\n",
    "\n",
    "# Check if it's Collaborative Space\n",
    "SPACE_NAME=$(jq .SpaceName /opt/ml/metadata/resource-metadata.json || exit 1)\n",
    "\n",
    "# if it's not a collaborative space \n",
    "if [ -z \"$SPACE_NAME\" ] || [ $SPACE_NAME == \"null\" ] ;\n",
    "then\n",
    "    # If it's a user-profile access\n",
    "    echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Domain Id ${DOMAIN_ID}\"\n",
    "    STUDIO_URL=\"https://${DOMAIN_ID}.studio.${REGION}.sagemaker.aws\"\n",
    "    \n",
    "# It is a collaborative space\n",
    "else\n",
    "\n",
    "    SEM=true\n",
    "    SPACE_ID=\n",
    "\n",
    "    # Check if Space Id was previously configured\n",
    "    if [ -f /tmp/space-metadata.json ]; then\n",
    "        SAVED_SPACE_ID=$(jq .SpaceId /tmp/space-metadata.json || exit 1)\n",
    "        SAVED_SPACE_ID=`sed -e 's/^\"//' -e 's/\"$//' <<< \"$SAVED_SPACE_ID\"`\n",
    "\n",
    "        if [ -z \"$SAVED_SPACE_ID\" ] || [ $SAVED_SPACE_ID == \"null\" ]; then\n",
    "            ASK_INPUT=true\n",
    "        else\n",
    "            ASK_INPUT=false\n",
    "        fi\n",
    "    else\n",
    "        ASK_INPUT=true\n",
    "    fi\n",
    "\n",
    "    # If Space Id is not available, ask for it\n",
    "    while [[ $SPACE_ID = \"\" ]] ; do\n",
    "        # If Space Id already configured, skeep the ask\n",
    "        if [ \"$ASK_INPUT\" = true ]; then\n",
    "            echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Please insert the Space Id from your url. e.g. https://${GREEN}<SPACE_ID>${NC}.studio.${REGION}.sagemaker.aws/jupyter/default/lab\"\n",
    "            read SPACE_ID\n",
    "            SEM=true\n",
    "        else\n",
    "            SPACE_ID=$SAVED_SPACE_ID\n",
    "        fi\n",
    "\n",
    "        if ! [ -z \"$SPACE_ID\" ] && ! [ $SPACE_ID == \"null\" ] ;\n",
    "        then\n",
    "            while $SEM; do\n",
    "                echo \"${SPACE_ID}\"\n",
    "                read -p \"Should this be used as Space Id? (y/N) \" yn\n",
    "                case $yn in\n",
    "                    [Yy]* )\n",
    "                        echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Domain Id ${DOMAIN_ID}\"\n",
    "                        echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Space Id ${SPACE_ID}\"\n",
    "\n",
    "                        jq -n --arg space_id $SPACE_ID '{\"SpaceId\":$space_id}' > /tmp/space-metadata.json\n",
    "\n",
    "                        STUDIO_URL=\"https://${SPACE_ID}.studio.${REGION}.sagemaker.aws\"\n",
    "\n",
    "                        SEM=false\n",
    "                        ;;\n",
    "                    [Nn]* ) \n",
    "                        SPACE_ID=\n",
    "                        ASK_INPUT=true\n",
    "                        SEM=false\n",
    "                        ;;\n",
    "                    * ) echo \"Please answer yes or no.\";;\n",
    "                esac\n",
    "            done\n",
    "        fi\n",
    "    done\n",
    "fi\n",
    "\n",
    "echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Studio Url ${STUDIO_URL}\"\n",
    "\n",
    "\n",
    "link=\"${STUDIO_URL}/jupyter/${RESOURCE_NAME}/proxy/${PORT}/\"\n",
    "\n",
    "echo -e \"${CYAN}${CURRENTDATE}: [INFO]:${NC} Starting Streamlit App\"\n",
    "echo -e \"${CYAN}${CURRENTDATE}: [INFO]: ${GREEN}${link}${NC}\"\n",
    "\n",
    "exit 0\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a2090-21cb-4bb8-9141-e2704ea491e9",
   "metadata": {},
   "source": [
    "# Streamlit 애플리케이션 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24a961-8722-43e1-9ec5-1305c9c8521b",
   "metadata": {},
   "source": [
    "- 아래와 같이 Streamlit을 위한 Python 라이브러리 설치 및 실행 스크립트를 실행합니다.\n",
    "<img src=\"images/streamlit-env.png\" width=\"600\"/>\n",
    "\n",
    "```bash\n",
    "cd aws-genai-for-retail/3_lab/\n",
    "```\n",
    "```bash\n",
    ". setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531469ff-a953-4e1c-9cc7-46e9cca5a711",
   "metadata": {},
   "source": [
    "- 아래와 같이 Streamlit 애플리케이션 실행을 위한 스크립트를 실행합니다.\n",
    "<img src=\"images/streamlit-exe.png\" width=\"1000\"/>\n",
    "\n",
    "```bash\n",
    ". run.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13deac32-d69f-4500-9289-998ed9b66eec",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 위 스크립트 실행 결과 중 마지막 Streamlit 실행 링크를 클릭하면 새로운 브라우저 탭에 아래와 같이 QA 서비스에 접속할 수 있습니다.\n",
    "<img src=\"images/streamlit-init.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6f435-a2d3-4569-bc0d-fcffb3c98400",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 질문을 입력하면 아래와 같이 LLM을 통해서 원하는 답을 얻을 수 있습니다.\n",
    "<img src=\"images/streamlit-chat.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ac50c-6a9d-4299-90cc-d16586472a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
